#https://www.kaggle.com/mlg-ulb/creditcardfraud/activity

library(mise)
print('teste') 
mise()
plot('x')
gc()
rm(list = ls())
options(scipen=999)#Não utilizar notação cientifica
options(digits=5)  

library(dplyr)
library(randomForest)
nfold_check<-function(y_test,y_model_prob,infoextra=c(),infoextra2=c()){
  df<-data.frame(ytest =y_test,yprobpred=y_model_prob )
  df<-df[sample(nrow(df)),]	
  print("------------------------------------------------------------------")
  print(infoextra)
  print(infoextra2)
  
  set.seed(123)
  p<-2.5
  n1<-1
  tabela_resumo <- data.frame(P=0,T0M0=0,T1M1=0,T1M0=0,T0M1=0)
  while (p<=99){
    tabela_resumo[n1,"P"]<-p/100
    tabela_resumo[n1,"T1M1"]<-NROW(df[df$yprobpred>=p/100 & df$ytest==1,])
    tabela_resumo[n1,"T1M0"]<-NROW(df[df$yprobpred<p/100 & df$ytest==1,])
    tabela_resumo[n1,"T0M0"]<-NROW(df[df$yprobpred<p/100 & df$ytest==0,])
    tabela_resumo[n1,"T0M1"]<-NROW(df[df$yprobpred>=p/100 & df$ytest==0,])
    n1<-n1+1
    p<-p+1.25
  }
  
  tabela_resumo$recall<- tabela_resumo$T1M1/(tabela_resumo$T1M1+tabela_resumo$T1M0)
  tabela_resumo$precision<- tabela_resumo$T1M1/(tabela_resumo$T1M1+tabela_resumo$T0M1)
  tabela_resumo$F1<- 2*(tabela_resumo$precision*tabela_resumo$recall)/(tabela_resumo$recall+tabela_resumo$precision)
  tabela_resumo$acuracia<-(tabela_resumo$T1M1+tabela_resumo$T0M0)/(tabela_resumo$T1M1+tabela_resumo$T0M0+tabela_resumo$T1M0+tabela_resumo$T0M1)
  tabela_resumo$abserrordif<-abs(tabela_resumo$T0M1-tabela_resumo$T1M0)
  tabela_resumo$MCC<-(tabela_resumo$T1M1*tabela_resumo$T0M0 - tabela_resumo$T0M1*tabela_resumo$T1M0)/
    sqrt((tabela_resumo$T1M1+tabela_resumo$T0M1)*(tabela_resumo$T1M1+tabela_resumo$T1M0)*
           (tabela_resumo$T0M0+tabela_resumo$T0M1)*(tabela_resumo$T0M0+tabela_resumo$T1M0))
  tabela_resumo$Jaccard<-(tabela_resumo$T1M1)/(tabela_resumo$T1M1+tabela_resumo$T1M0+tabela_resumo$T0M1)
  
  #coloca NA em dados infinitos
  tabela_resumo[mapply(is.infinite, tabela_resumo)] <- 0
  
  tabela_resumo[,c(1,9)]
  #points(tabela_resumo[,c('P','recall')],col="green", xlim=c(0,1), ylim=c(0,1))
  #points(tabela_resumo[,c('P','F1')],col="red", xlim=c(0,1), ylim=c(0,1))
  #points(tabela_resumo[,c('P','precision')],col="blue", xlim=c(0,1), ylim=c(0,1))
  #points(tabela_resumo[,c('P','acuracia')],col="yellow", xlim=c(0,1), ylim=c(0,1))
  
  plot(x=tabela_resumo[,'P'], y=tabela_resumo[,'F1'], main = "F1",
       xlab = "Cutoff", ylab = "%", xlim = c(0,1), ylim = c(0,1),pch = 19, frame = FALSE,col="green")
  
  cutoff_indicado<-mean(tabela_resumo[tabela_resumo[,8]==max(tabela_resumo[,8],na.rm=TRUE),1])
  cutoff_indicado1<-mean(tabela_resumo[tabela_resumo[,9]==max(tabela_resumo[,9],na.rm=TRUE),1])
  cutoff_indicado2<-mean(tabela_resumo[tabela_resumo[,10]==min(tabela_resumo[,10],na.rm=TRUE),1])
  cutoff_indicado3<-mean(tabela_resumo[tabela_resumo[,11]==max(tabela_resumo[,11],na.rm=TRUE),1])
  
  if(is.na(cutoff_indicado)){cutoff_indicado<-0}
  if(is.na(cutoff_indicado1)){cutoff_indicado1<-0}
  if(is.na(cutoff_indicado2)){cutoff_indicado2<-0}
  if(is.na(cutoff_indicado3)){cutoff_indicado3<-0}
  
  
  for(cutoff_uso in c(cutoff_indicado)){#,cutoff_indicado1,cutoff_indicado2,cutoff_indicado3
    
    if(cutoff_indicado==cutoff_uso){
      print(paste('Cutoff indicado (F1 Máximo): ',cutoff_uso))
    }else{
      if(cutoff_indicado1==cutoff_uso){
        print(paste('Cutoff indicado (Acurácia Máxima): ',cutoff_uso))
      }else{
        if(cutoff_indicado2==cutoff_uso){
          print(paste('Cutoff indicado (Balanceado): ',cutoff_uso))
        }else{
          if(cutoff_indicado3==cutoff_uso){
            print(paste('Cutoff indicado (MCC): ',cutoff_uso))
          }
        }
      }
    }
    
    Union_table<-data.frame(TARGET =df$ytest,PRED_DEF=ifelse(df$yprobpred>=cutoff_uso,1,0) )
    print(prop.table(table(Union_table[,c('TARGET','PRED_DEF')]))*100)
    print(table(Union_table[,c('TARGET','PRED_DEF')]))
    
    tabela_resumo <- data.frame(n=0,T0M0=0,T1M1=0,T1M0=0,T0M1=0)
    folds <- cut(seq(1,nrow(df)),breaks=10,labels=FALSE)	
    #Perform 10 fold cross validation
    for(i in 1:10){
      #Segment your data by fold using the which() function 
      testIndexes <- which(folds==i,arr.ind=TRUE)
      df_temp<-df[testIndexes,]
      
      tabela_resumo[i,"n"]<-i
      tabela_resumo[i,"T1M1"]<-NROW(df_temp[df_temp$yprobpred>=cutoff_uso & df_temp$ytest==1,])
      tabela_resumo[i,"T1M0"]<-NROW(df_temp[df_temp$yprobpred<cutoff_uso & df_temp$ytest==1,])
      tabela_resumo[i,"T0M0"]<-NROW(df_temp[df_temp$yprobpred<cutoff_uso & df_temp$ytest==0,])
      tabela_resumo[i,"T0M1"]<-NROW(df_temp[df_temp$yprobpred>=cutoff_uso & df_temp$ytest==0,])
    }
    
    tabela_resumo$recall<- tabela_resumo$T1M1/(tabela_resumo$T1M1+tabela_resumo$T1M0)
    tabela_resumo$precision<- tabela_resumo$T1M1/(tabela_resumo$T1M1+tabela_resumo$T0M1)
    tabela_resumo$F1<- 2*(tabela_resumo$precision*tabela_resumo$recall)/(tabela_resumo$recall+tabela_resumo$precision)
    tabela_resumo$Acuracia<-(tabela_resumo$T1M1+tabela_resumo$T0M0)/(tabela_resumo$T1M1+tabela_resumo$T0M0+tabela_resumo$T1M0+tabela_resumo$T0M1)
    tabela_resumo$MCC<-(tabela_resumo$T1M1*tabela_resumo$T0M0 - tabela_resumo$T0M1*tabela_resumo$T1M0)/
      sqrt((tabela_resumo$T1M1+tabela_resumo$T0M1)*(tabela_resumo$T1M1+tabela_resumo$T1M0)*
             (tabela_resumo$T0M0+tabela_resumo$T0M1)*(tabela_resumo$T0M0+tabela_resumo$T1M0))
    tabela_resumo$Jaccard<-(tabela_resumo$T1M1)/(tabela_resumo$T1M1+tabela_resumo$T1M0+tabela_resumo$T0M1)
    tabela_resumo$TP<-tabela_resumo$T1M1
    tabela_resumo$TN<-tabela_resumo$T0M0
    tabela_resumo$FP<-tabela_resumo$T0M1
    tabela_resumo$FN<-tabela_resumo$T1M0
    tabela_resumo<-tabela_resumo[,c("n","Acuracia","F1","recall","precision","MCC","Jaccard","TP","TN","FP","FN")]
    
    tabela_resumo[11,1]<-'Mean'
    tabela_resumo[11,2]<-mean(tabela_resumo[1:10,2],na.rm=TRUE)
    tabela_resumo[11,3]<-mean(tabela_resumo[1:10,3],na.rm=TRUE)
    tabela_resumo[11,4]<-mean(tabela_resumo[1:10,4],na.rm=TRUE)
    tabela_resumo[11,5]<-mean(tabela_resumo[1:10,5],na.rm=TRUE)
    tabela_resumo[11,6]<-mean(tabela_resumo[1:10,6],na.rm=TRUE)
    tabela_resumo[11,7]<-mean(tabela_resumo[1:10,7],na.rm=TRUE)
    tabela_resumo[11,8]<-sum(tabela_resumo[1:10,8],na.rm=TRUE)
    tabela_resumo[11,9]<-sum(tabela_resumo[1:10,9],na.rm=TRUE)
    tabela_resumo[11,10]<-sum(tabela_resumo[1:10,10],na.rm=TRUE)
    tabela_resumo[11,11]<-sum(tabela_resumo[1:10,11],na.rm=TRUE)
    
    tabela_resumo[12,1]<-'sd'
    tabela_resumo[12,2]<-sd(tabela_resumo[1:10,2],na.rm=TRUE)
    tabela_resumo[12,3]<-sd(tabela_resumo[1:10,3],na.rm=TRUE)
    tabela_resumo[12,4]<-sd(tabela_resumo[1:10,4],na.rm=TRUE)
    tabela_resumo[12,5]<-sd(tabela_resumo[1:10,5],na.rm=TRUE)
    tabela_resumo[12,6]<-sd(tabela_resumo[1:10,6],na.rm=TRUE)
    tabela_resumo[12,7]<-sd(tabela_resumo[1:10,7],na.rm=TRUE)
    print(tabela_resumo[11:12,])
    print("------------------------------------------------------------------")
  }
  tabela_resumo$cutoff_F1<-cutoff_uso
  resumo<-tabela_resumo[11:12,]
 return(resumo) 
}
undersampling_byclusters<-function(x,TARGET=c(),groupby=3,exclude_feature=c(),noise_filter=c(0.025,0.975),n_balance=600,N_min_group=60,seed=123,add_idsample=F,rm_bychsquare_anova=T,rm_byIV=F){
  
  rename_columns<-function(df,caracteres=c('.',',',';',' ','~','*','[',']','{','}','(',')','&')){
    for (i in 1:NCOL(df)){for(j in caracteres){names(df)[i]<-gsub(paste0("\\",j),"",names(df)[i])}}
    return(df)
  }
  dtini<-Sys.time()
  
  cluster_max<-8
  decimal_places<-4
  colunas_semclusters<-c()
  dataset_temp<-x
  dataset_temp$cluster_principal<-''
  dataset_temp<-rename_columns(dataset_temp)
  
  #insere 90909090 para dados nulos
  for(i in 1:ncol(dataset_temp)){
    dataset_temp[is.na(dataset_temp[,i]),i]<-90909090}
  
  #verifica se a variável TARGET é dummy(classificação)
  if(length(TARGET)==0){
    perce_original <- c()
  }else{
    if(((NROW(dataset_temp[dataset_temp[,TARGET]==0,])+NROW(dataset_temp[dataset_temp[,TARGET]==1,]))==NROW(dataset_temp))){
      perce_original <- NROW(dataset_temp[dataset_temp[,TARGET]==1,])/NROW(dataset_temp)
    }else{
      perce_original <- c()}
  }
  
  #identifica colunas iniciais, as numéricas, remove as não desejadas definidas 
  colunas_iniciais<-colnames(dataset_temp)
  dataset_temp$cluster<-''
  colunas_numericas <- colnames(dataset_temp[,unlist(lapply(dataset_temp, is.numeric))])
  colunas_numericas <- setdiff(colunas_numericas,exclude_feature)
  
  if(is.numeric(groupby)){
    groupby_temp<-c()
    for(coluna_num in colunas_numericas){
      txt<-paste0(coluna_num , ';' , groupby)
      groupby_temp<-c(groupby_temp,txt)
    }
    groupby<-groupby_temp
  }
  
  
  #Se existir o conjunto de variáveis e número de clusters, separa para conhecer a variáveis que vão compor o cluster principal
  if(length(groupby)>0){
    for (coluna in groupby){
      coluna<-unlist(strsplit(coluna,';'))
      colunas_semclusters<-c(colunas_semclusters,coluna[1])}
  }else{
    #se não forem definidas as variáveis e seus clusters, todas variáveis numéricas serão amostradas
    colunas_semclusters<-colunas_numericas}
  
  print(groupby)
  
  
  #Apenas para fazer um print dos clusters gerados individualmente
  itemp<-1
  for (coluna in groupby){		
    #se separado com ; existe o número de cluster definido
    ncluster_definido<-unlist(strsplit(coluna,';'))[2]
    n_clusters<-as.numeric(ncluster_definido)
    coluna<-unlist(strsplit(coluna,';'))[1]		
    
    print(coluna)
    mydata <- scale(dataset_temp[,coluna,drop=FALSE])
    
    set.seed(seed)
    n_clusters<-apply(mydata, 2, function(x) ifelse(length(unique(x))>=n_clusters,n_clusters,length(unique(x))))
    datasetCluster <- kmeans(mydata, centers = n_clusters, iter.max = 100, nstart = 17,algorithm = c("Hartigan-Wong"))
    if(datasetCluster$ifault==4){
      datasetCluster<-mydata
      datasetCluster$cluster <-ifelse(mydata[,coluna]<=median(mydata[,coluna]),1,2)}
    
    dataset_temp$cluster_temp<-paste(coluna,"."	,datasetCluster$cluster,";", sep="")
    dataset_temp$cluster <- paste(dataset_temp$cluster, dataset_temp$cluster_temp, sep="")
    
    if((coluna %in% colunas_numericas)){
      formula_aggregate<- as.formula(paste("cbind(",TARGET,",",coluna,") ~ cluster_temp"))
      detalhe_cluster<- aggregate(formula_aggregate, data=dataset_temp,FUN= {function(x) c(avg=mean(x), sd=sd(x), count=length(x),min=min(x),max=max(x),sum=sum(x))})
      detalhe_cluster<-data.frame(cluster=detalhe_cluster$cluster_temp,avg=format(round(detalhe_cluster[,coluna][,1], decimal_places), nsmall = decimal_places),sd=format(round(detalhe_cluster[,coluna][,2], decimal_places), nsmall = decimal_places),qtde=detalhe_cluster[,coluna][,3],
                                  min_max=paste(coluna,format(round(detalhe_cluster[,coluna][,4], decimal_places), nsmall = decimal_places),'~',format(round(detalhe_cluster[,coluna][,5], decimal_places), nsmall = decimal_places),';', sep=""),
                                  tg_avg=format(round(detalhe_cluster[,TARGET][,1], decimal_places), nsmall = decimal_places),tg_sum=detalhe_cluster[,TARGET][,6],stringsAsFactors = FALSE)
      
      detalhe_cluster$Variable<-coluna
      #Realizaçao do teste QuiQuadrado
      if(length(perce_original)>0){
        detalhe_cluster$esperado<- sum(detalhe_cluster$tg_sum)/sum(detalhe_cluster$qtde)*detalhe_cluster$qtde
        detalhe_cluster$chiquadrado_temp<-((abs(detalhe_cluster$tg_sum-detalhe_cluster$esperado))^2)/detalhe_cluster$esperado
        detalhe_cluster$chiquadrado<-round(sum(detalhe_cluster$chiquadrado_temp),4)
        detalhe_cluster$chiquadrado_crit99<-round(qchisq(.99, df=(NROW(detalhe_cluster)-1)),4)
        detalhe_cluster$P<-round(pchisq(detalhe_cluster$chiquadrado, df=(NROW(detalhe_cluster)-1), lower.tail=FALSE),4)
        detalhe_cluster$P0<-(detalhe_cluster$qtde*(1-as.numeric(detalhe_cluster$tg_avg))+0.5)/sum(detalhe_cluster$qtde*(1-as.numeric(detalhe_cluster$tg_avg)))
        detalhe_cluster$P1<-(detalhe_cluster$qtde*as.numeric(detalhe_cluster$tg_avg)+0.5)/sum(detalhe_cluster$qtde*as.numeric(detalhe_cluster$tg_avg))
        detalhe_cluster$AdjustedWOE<-log(detalhe_cluster$P0/detalhe_cluster$P1)
        detalhe_cluster$IV<-sum(detalhe_cluster$AdjustedWOE*(detalhe_cluster$P0-detalhe_cluster$P1))
        detalhe_cluster$IV_Power<-ifelse(detalhe_cluster$IV<0.02,'Useless',ifelse(detalhe_cluster$IV<0.1,'Weak',ifelse(detalhe_cluster$IV<0.3,'Medium',ifelse(detalhe_cluster$IV<0.5,'Strong','Too good'))))
        detalhe_cluster[,c('chiquadrado_temp','esperado','P1','P0','AdjustedWOE')]<-NULL
        
      }else{#regressor -> ANOVA
        detalhe_cluster$C<- (sum(dataset_temp[,TARGET])^2)/NROW(dataset_temp)
        detalhe_cluster$SQT<- sum(dataset_temp[,TARGET]^2)-detalhe_cluster$C
        detalhe_cluster$SQTr<- sum((detalhe_cluster$tg_sum^2)/detalhe_cluster$qtde)-detalhe_cluster$C
        detalhe_cluster$SQR<- detalhe_cluster$SQT-detalhe_cluster$SQTr
        detalhe_cluster$QMTr<- detalhe_cluster$SQTr/(NROW(detalhe_cluster)-1)
        detalhe_cluster$QMR<- detalhe_cluster$SQR/(sum(detalhe_cluster$qtde)-NROW(detalhe_cluster)-1)
        detalhe_cluster$FSnedecor<-detalhe_cluster$QMTr/detalhe_cluster$QMR
        detalhe_cluster$FCriticalValue<- qf(.99, df1=(NROW(detalhe_cluster)-1), df2=sum(detalhe_cluster$qtde))
        detalhe_cluster$P<- (1-pf(mean(detalhe_cluster$FSnedecor), df1=(NROW(detalhe_cluster)-1), df2=sum(detalhe_cluster$qtde)))
        detalhe_cluster[,c('C','SQR','SQT','SQTr','QMTr','QMR')]<-NULL
      }
      
      if(!exists("acumulador_infocluster")){
        acumulador_infocluster<-detalhe_cluster
      }else{
        acumulador_infocluster<-rbind(acumulador_infocluster,detalhe_cluster)}
      
      #se o teste quiquadrado idenficar irrelevancia, não adiciona o cluster
      id_rm<-F
      if(rm_bychsquare_anova==T & max(detalhe_cluster$P)>0.05){id_rm<-T}
      
      #remove cluster por IV
      if(id_rm==F & rm_byIV==T){
        if(length(perce_original)>0){
          if(max(detalhe_cluster$IV)<0.1){
            id_rm<-T}}}
      
      
      if(id_rm==F){
        dataset_temp[,paste0("V_",itemp)]<-paste(coluna,"."	,datasetCluster$cluster,";", sep="")
        itemp<-itemp+1
        dataset_temp$cluster_principal <- paste(dataset_temp$cluster_principal, dataset_temp$cluster_temp, sep="")}
      
    }
    
    gc()
    dataset_temp$cluster_temp<-NULL	
    rm(mydata,datasetCluster)
    
  }
  if(length(perce_original)>0){print(unique(acumulador_infocluster[,c('Variable','IV_Power')]))}
  detalhe_cluster$Variable<-NULL
  print(acumulador_infocluster)
  
  itemp<-itemp-1
  new_columns_names<-paste0("V_",seq(1:itemp))
  dataset_temp$um<-1	
  dataset_temp$ID_AMOSTRA<-0
  dataset_temp$cluster_temp<-1
  cluster_temp<-c(1)
  
  
  NROW0<-NROW(dataset_temp[dataset_temp[,TARGET]==0,])
  NROW1<-NROW(dataset_temp[dataset_temp[,TARGET]==1,])
  dataset_temp[dataset_temp[,TARGET]==0,'rownum0']<-sample(1:NROW0, NROW0, replace=FALSE)
  dataset_temp[dataset_temp[,TARGET]==0 & dataset_temp[,'rownum0']<=NROW1,'ID_AMOSTRA']<-1
  dataset_temp[dataset_temp[,TARGET]==1,'ID_AMOSTRA']<-1
  dataset_temp[,'rownum0']<-NULL
  
  if(length(perce_original)>0){
    print("Proporcao original...")
    print(cbind(freq=table(dataset_temp[,TARGET]), perc=prop.table(table(dataset_temp[,TARGET]))*100))}
  
  if(length(perce_original)>0){
    print("Após selecao randomizada simples e balanceada...")
    print(cbind(freq=table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]), perc=prop.table(table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]))*100))}
  
  
  for(i in new_columns_names){
    formula_aggregate  <- as.formula(paste("um", "~", i))	
    qtde_groupby<- aggregate(formula_aggregate, data=dataset_temp,FUN=sum)
    vetor_groupby<- qtde_groupby[order(qtde_groupby[,2]),1]
    cluster_n<- NROW(qtde_groupby)#qtde de cluster de cada variável, por vir da agregaçao, posso contar as linhas

    for (j in vetor_groupby){
      print(j)
      n_balance_loop<-n_balance
      vetor0<-dataset_temp[,i]==j
      NROW_LOOP<-NROW(dataset_temp[vetor0,])
      n_balance_loop<-ifelse(n_balance_loop>NROW_LOOP,NROW_LOOP,n_balance_loop)
      
      vetor_temp_nao_amostrado<-vetor0 & dataset_temp[,'ID_AMOSTRA']==0
      NROW_LOOP0<-sum(vetor_temp_nao_amostrado)
      NROW_LOOP1<-sum(vetor0 & dataset_temp[,'ID_AMOSTRA']>0)
      dataset_temp[vetor_temp_nao_amostrado,'rownum0']<-sample(1:NROW_LOOP0, NROW_LOOP0, replace=FALSE)
      
      if(NROW_LOOP1<n_balance_loop){
          vetor_rownum<-vetor_temp_nao_amostrado & dataset_temp[,'rownum0']<=(n_balance_loop-NROW_LOOP1)
          soma_rownum<-sum(vetor_rownum)
          dataset_temp[vetor_rownum,'ID_AMOSTRA']<-sample(soma_rownum:1/soma_rownum)}
    }		
  }
  rm(vetor_rownum,vetor0)
  dataset_temp[dataset_temp[,TARGET]==1,'ID_AMOSTRA']<-1
  dataset_temp$cluster_temp<-NULL
  
  if(length(perce_original)>0){
    print("Após amostragem clusterizada...")
    print(cbind(freq=table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]), perc=prop.table(table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]))*100))}
  
  
  #Utilizado para pegar combinação dos cluster com poucos dados, utilizado um pouco antes da final da função 
  df_cluster<- aggregate(as.formula(paste("cbind(",TARGET,",ID_AMOSTRA) ~ cluster_principal")),data=dataset_temp,FUN={function(x) c(count=length(x),avg=mean(x),soma=sum(ifelse(x>0,1,0)))})
  df_cluster<- data.frame(cluster_principal=df_cluster$cluster_principal,count=df_cluster[,TARGET][,1],avg=df_cluster[,TARGET][,2],sum_sample=df_cluster[,"ID_AMOSTRA"][,3],stringsAsFactors=FALSE)
  qtde_clusters<-NROW(df_cluster)
  df_cluster$cluster_simples<-1:qtde_clusters #mudança da chave complexa do cluster para um número mais simples
  dataset_temp<-merge(dataset_temp,df_cluster)
  #print(df_cluster[order(-df_cluster[,'sum_sample']),])
  print(paste0(qtde_clusters," clusters combinados"))
  
  
  #aplicaçao da agregação identificada acima para pegar N_min_group amostras
  if(length(perce_original)>0){
    vetor_redundancia<-df_cluster[df_cluster[,'sum_sample']>N_min_group & df_cluster[,3]<=noise_filter[1],'cluster_simples'] #classificador
  }else{
    vetor_redundancia<-df_cluster[df_cluster[,'sum_sample']>N_min_group,'cluster_simples']} #regressor
  
  for(cluster_loop in vetor_redundancia){
    
    if(length(perce_original)>0){
      vetor_n_amostra<-dataset_temp[,'cluster_simples'] == cluster_loop & dataset_temp[,'ID_AMOSTRA']>0 & dataset_temp[,TARGET] == 0
    }else{
      vetor_n_amostra<-dataset_temp[,'cluster_simples'] == cluster_loop & dataset_temp[,'ID_AMOSTRA']>0}
    n_amostra<-sum(vetor_n_amostra)
    
    dataset_temp[vetor_n_amostra,'rownumber']<-sample(1:n_amostra, n_amostra, replace=FALSE)
    dataset_temp[vetor_n_amostra & dataset_temp[,'rownumber']>N_min_group,'ID_AMOSTRA']<-0
    
    #correcao do já que algumas amostras foram excluidas
    if(length(perce_original)>0){
      vetor_n_amostra<-dataset_temp[,'cluster_simples'] == cluster_loop & dataset_temp[,'ID_AMOSTRA']>0 & dataset_temp[,TARGET] == 0
    }else{
      vetor_n_amostra<-dataset_temp[,'cluster_simples'] == cluster_loop & dataset_temp[,'ID_AMOSTRA']>0}
    soma_n_amostra<-sum(vetor_n_amostra)
    dataset_temp[vetor_n_amostra,'ID_AMOSTRA']<-(soma_n_amostra:1)/soma_n_amostra
    rm(vetor_n_amostra)
    
  }
  rm(vetor_redundancia)
  
  if(length(perce_original)>0){
    print("Após eliminação de redundância e possível incremento de registros...")
    print(cbind(freq=table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]), perc=prop.table(table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]))*100))}
  
  #limpar ruido
  if(length(perce_original)>0){
    
    if(mean(df_cluster[df_cluster[,3]>0,3])>noise_filter[1]){#Somente se a média de defeito dos clusters que o tiveram forem acima do ruído, então aplicar o noise reduction inferior, 
      #noise reduction LOW
      for(cluster_loop in df_cluster[df_cluster[,3]>0 & df_cluster[,3]<=noise_filter[1],1]){
        dataset_temp[dataset_temp[,'cluster_principal'] == cluster_loop & dataset_temp[,TARGET]==1,'ID_AMOSTRA']<-0}
      
      print("Após eliminacao ruidos em grupos predominantemente sem defeitos")
      print(cbind(freq=table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]), perc=prop.table(table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]))*100))}
    
    #noise reduction high
    for(cluster_loop in df_cluster[df_cluster[,3]>=noise_filter[2],1]){
      dataset_temp[dataset_temp[,'cluster_principal'] == cluster_loop & dataset_temp[,TARGET]==0,'ID_AMOSTRA']<-0}
    
    print("Após eliminacao ruidos em grupos predominantemente com defeitos")
    print(cbind(freq=table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]), perc=prop.table(table(dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,TARGET]))*100))
  }
  rm(df_cluster)
  
  if(add_idsample==T){colunas_iniciais<-c(colunas_iniciais,'ID_AMOSTRA')}
  dataset_temp<-dataset_temp[dataset_temp[,'ID_AMOSTRA']>0,]
  dataset_temp$ID_AMOSTRA<-(1-dataset_temp$ID_AMOSTRA)
  
  dataset_temp<-dataset_temp[,colunas_iniciais]
  dataset_temp <-dataset_temp[,c(setdiff(colnames(dataset_temp),TARGET),TARGET)]
  print(paste0(dtini,' ~ ',Sys.time()))
  
  #identifica colunas com dados identicos(zerovar)
  foo <- function(dat) {
    out <- lapply(dat, function(x) length(unique(x)))
    want <- which(!out > 1)
    unlist(want)
  }
  zerovar<-names(foo(dataset_temp))
  colunas_uteis<-setdiff(colunas_iniciais,zerovar)
  dataset_temp<-dataset_temp[,colunas_uteis]
  
  
  #defaz o que foi feito no início para os dados nulos
  for(i in 1:ncol(dataset_temp)){dataset_temp[dataset_temp[,i]==90909090,i]<-NA}
  print(dim(dataset_temp))
  dtfim<-Sys.time()
  tempo_processamento <-paste0('Start in:', dtini,', Finish in:',dtfim)
  cat(tempo_processamento)
  return(dataset_temp)	
}
syntetic_sampling<-function(df,groupby='cluster_principal',n_groupby_tg=10,TARGET=c(),semente=123){
  statmod <- function(x) {
    z <- table(as.vector(x))
    names(z)[z == max(z)]
  }
  set.seed(semente)
  dataset_temp<-df
  print('Create syntetic samples...')
  
  #verifica se a variável TARGET é dummy(classificação)
  if(length(TARGET)==0){
    perce_original <- c()
  }else{
    if(((NROW(dataset_temp[dataset_temp[,TARGET]==0,])+NROW(dataset_temp[dataset_temp[,TARGET]==1,]))==NROW(dataset_temp))){
      perce_original <- NROW(dataset_temp[dataset_temp[,TARGET]==1,])/NROW(dataset_temp)
    }else{
      perce_original <- c()}
  }
  
  
  #utilizado para gerar a média na criação das amostras
  numericalcol<- rownames(data.frame(which(sapply( dataset_temp, class ) == 'numeric' )))
  
  dataset_temp$um<-1
  qtde_groupby<- aggregate(as.formula(paste("um", "~", groupby)), data=dataset_temp,FUN=sum)
  vetor_group <-qtde_groupby[qtde_groupby[,"um"]<n_groupby_tg ,1]
  dataset_temp$um<-NULL
  
  dataset_temp_out <- dataset_temp[!(dataset_temp[,groupby] %in% vetor_group),]
  dataset_temp_in <- dataset_temp[dataset_temp[,groupby] %in% vetor_group,]
  
  for (i in vetor_group){
    if(length(groupby)>0){
      
      #para fazer amostra sintética do não defeito tem que acontecer 5 vezes!
      temp_sum<-0
      for(j in 1:5){
        VLR_TG <-max(sample_n(dataset_temp_in[dataset_temp_in[,groupby]==i,TARGET,drop=F],1))
        temp_sum<-temp_sum+VLR_TG}
        dataset_temp<-dataset_temp_in[dataset_temp_in[,groupby]==i & dataset_temp_in[,TARGET]==ifelse(temp_sum==0,0,1),]
      
    }else{
      dataset_temp<-dataset_temp_in[dataset_temp_in[,groupby]==i,]}
    
    ng_df<-NROW(dataset_temp)
    l<-ng_df
    
    #Não necessário fazer amostra sintética de Não defeito
    ndupl_anterior<-0
    while( l<n_groupby_tg & l>1){ 
      
      dataset_temp2<-dataset_temp
      NMIN<-2
      NMAX<-NROW(dataset_temp2)
      NMAX<-ifelse(NMAX>=30,30,NMAX)			
      NMIN<-ifelse(NMAX==1,1,NMIN)
      
      # deixa aleatório as amostras no dataset temporário
      #Não usar uma qtde maior que a disponível
      qtde_linhas <- round(runif(1, min=NMIN ,max =NMAX))#n_TRIGGER)) #escolhe o tamanho da amostra (2 no mínimo)
      dataset_temp2<-sample_n(dataset_temp2,qtde_linhas)
      
      #transforma o dataset em um de uma linha com média das colunas numéricas
      for(c in numericalcol){				
        #se variável for dummy, coloca a moda
        if((NROW(dataset_temp2[dataset_temp2[,c]==1,c])+NROW(dataset_temp2[dataset_temp2[,c]==0,c]))==NROW(dataset_temp2)){
          dataset_temp2[,c]<-as.numeric(statmod(dataset_temp2[,c]))
        }else{#Senão coloca a média
          dataset_temp2[,c]<-round(mean(dataset_temp2[,c]),5)
        }				
      } 
      dataset_temp2<-dataset_temp2[1,]#todas linhas são iguais a média, escolhe apenas 1
      
      #acumula no dataframe syntetic_temp
      if(exists("syntetic_temp0")){
        syntetic_temp0<-rbind(syntetic_temp0,dataset_temp2)
      }else{
        syntetic_temp0<-dataset_temp2}
      
      ndupl<-NROW(syntetic_temp0[duplicated(syntetic_temp0[,numericalcol]),])
      l<-l+ifelse(ndupl>=1 & ndupl>ndupl_anterior,n_groupby_tg,1)
      ndupl_anterior<-ndupl
    }
    
    if(exists("syntetic_temp")){
      if(exists("syntetic_temp0")){
        syntetic_temp<-rbind(syntetic_temp,syntetic_temp0,dataset_temp)
        rm(syntetic_temp0)
      }else{
        syntetic_temp<-rbind(syntetic_temp,dataset_temp)}
    }else{
      if(exists("syntetic_temp0")){
        syntetic_temp<-rbind(syntetic_temp0,dataset_temp)
        rm(syntetic_temp0)
      }else{
        syntetic_temp<-dataset_temp}}
  }
  
  if(exists("syntetic_temp")){
    dataset_cluster_final<-rbind(syntetic_temp,dataset_temp_out)
  }else{
    dataset_cluster_final<-dataset_temp_out}
  if(length(groupby)>0){
    print(cbind(freq=table(dataset_cluster_final[,TARGET]), perc=prop.table(table(dataset_cluster_final[,TARGET]))*100))
  }else{
    print(dim(dataset_cluster_final))
  }
  return(dataset_cluster_final)
}

#--------------------------------------
#dataframe com dados aleatórios com n registros
set.seed(1)
comparacao_dados<-data.frame(n="",Acuracia=0.0,F1=0.0,recall=0.0,precision=0.0,MCC=0.0,Jaccard=0.0,TP=0,TN=0.0,FP=0.0,FN=0.0,cutoff_F1=0.0,tipo_amostragem="",intervalo="")

#Carregando a base dados
BD ='D:/creditcard.csv' 
data_set <- read.table(file = BD, header = TRUE, sep=',',dec = ".", stringsAsFactors = FALSE)

#Define variável de interesse----------------------
TARGET<-c('Class')

#Retira  amostra da populacao bruta----------------
splits<-runif(nrow(data_set))<=0.25
dataset_amostra<-data_set[splits,]
data_set<-data_set[!splits,]

#seleciona apenas dados numéricos
TRAIN_VECTOR<- rownames(data.frame(which(sapply( data_set, class ) == 'numeric' )))
TRAIN_VECTOR<-setdiff(TRAIN_VECTOR,TARGET)


#treino com na base integral --------------------------------------
tipo_amostragem<-paste0("Amostra original ",NROW(data_set[data_set[,TARGET]==1,]), "/",NROW(data_set),"=",round(NROW(data_set[data_set[,TARGET]==1,])/NROW(data_set)*100,2),"%")
dtini<-Sys.time()
classifier <- randomForest(x = data_set[,TRAIN_VECTOR],y = factor(ifelse(data_set[,TARGET]==1, 'Y', 'N')),keep.forest=TRUE)#,importance=TRUE)
dtfim<-Sys.time()
y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
intervalo<-paste0(dtini,' ~ ',dtfim)
comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))

#--------------------------------------
#Amostragem aleatória simples----------------
df_aletsimples<-rbind(sample_n(data_set[data_set[,TARGET]==0,],NROW(data_set[data_set[,TARGET]==1,]),replace=FALSE),data_set[data_set[,TARGET]==1,])

#Amostragem aleatória simples n equivalente----------------
df_aletsimplesnevq<-rbind(sample_n(data_set[data_set[,TARGET]==0,],7000,replace=FALSE),data_set[data_set[,TARGET]==1,])

#amostragem smote----------------
library(DMwR)
data_smote<-data_set
data_smote[,TARGET] <- as.factor(data_smote[,TARGET])
data_smote <- SMOTE(as.formula(paste(TARGET, "~", paste(TRAIN_VECTOR, collapse="+"))),data= data_smote,k=5, perc.over = 200, perc.under=200)
data_smote[,TARGET]<-as.numeric(levels(data_smote[,TARGET]))[data_smote[,TARGET]]
##AMostragem clusterizada--------------------------------------
#groupby =3, significa 3 clusters por variável para todas as variáveis... se houver menos clusters, a função diminui para não dar erro (exemplo dummy)

#amostragem ROSE----------------
library(ROSE)
dataset_rose <- ROSE(as.formula(paste(TARGET, "~", paste(TRAIN_VECTOR, collapse="+"))), data = data_set,N=10000, seed = 1)$data


#Modelo aleatória simples n equivalente  --------------------------------------
tipo_amostragem<-paste0("Aleatória simples n equiv ",NROW(df_aletsimplesnevq[df_aletsimplesnevq[,TARGET]==1,]), "/",NROW(df_aletsimplesnevq),"=",round(NROW(df_aletsimplesnevq[df_aletsimplesnevq[,TARGET]==1,])/NROW(df_aletsimplesnevq)*100,2),"%")
dtini<-Sys.time()
classifier <- randomForest(x = df_aletsimplesnevq[,TRAIN_VECTOR],y = factor(ifelse(df_aletsimplesnevq[,TARGET]==1, 'Y', 'N')),keep.forest=TRUE)#,importance=TRUE)
dtfim<-Sys.time()
y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
intervalo<-paste0(dtini,' ~ ',dtfim)
comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))


#Modelo com treinamento baseado em balanceamento aleatório simples--------------------------------------
tipo_amostragem<-paste0("Aleatória Simples ",NROW(df_aletsimples[df_aletsimples[,TARGET]==1,]), "/",NROW(df_aletsimples),"=",round(NROW(df_aletsimples[df_aletsimples[,TARGET]==1,])/NROW(df_aletsimples)*100,2),"%")
dtini<-Sys.time()
classifier <- randomForest(x = df_aletsimples[,TRAIN_VECTOR],y = factor(ifelse(df_aletsimples[,TARGET]==1, 'Y', 'N')),keep.forest=TRUE)#,importance=TRUE)
dtfim<-Sys.time()
y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
intervalo<-paste0(dtini,' ~ ',dtfim)
comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))


#--------------------------------------
#Modelo com treinamento baseado em balanceamento SMOTE
tipo_amostragem<-paste0("SMote ",NROW(data_smote[data_smote[,TARGET]==1,]), "/",NROW(data_smote),"=",round(NROW(data_smote[data_smote[,TARGET]==1,])/NROW(data_smote)*100,2),"%")
dtini<-Sys.time()
classifier <- randomForest(x = data_smote[,TRAIN_VECTOR],y = factor(ifelse(data_smote[,TARGET]==1, 'Y', 'N')),keep.forest=TRUE)#,importance=TRUE)
dtfim<-Sys.time()
y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
intervalo<-paste0(dtini,' ~ ',dtfim)
comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))


#--------------------------------------
#Modelo com treinamento baseado em balanceamento ROSE
tipo_amostragem<-paste0("ROSE ",NROW(dataset_rose[dataset_rose[,TARGET]==1,]), "/",NROW(dataset_rose),"=",round(NROW(dataset_rose[dataset_rose[,TARGET]==1,])/NROW(dataset_rose)*100,2),"%")
dtini<-Sys.time()
classifier <- randomForest(x = dataset_rose[,TRAIN_VECTOR],y = factor(ifelse(dataset_rose[,TARGET]==1, 'Y', 'N')),keep.forest=TRUE)#,importance=TRUE)
dtfim<-Sys.time()
y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
intervalo<-paste0(dtini,' ~ ',dtfim)
comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))


data_set1<-undersampling_byclusters(x=data_set,TARGET=TARGET,groupby=5,add_idsample=T,n_balance=600,N_min_group=60)
data_set<-data_set1
#Modelo com treinamento baseado em amostragem clusterizada variando sua amostra conforme percentuais definidos 0.05, 0.10 e ...
for(i in c(1.0)){
  print(paste(i*100,"% da amostra clusterizada original"))
  tipo_amostragem<-paste0(i*100,"% Cluster ",NROW(data_set[data_set[,TARGET]==1,]), "/",NROW(data_set[data_set$ID_AMOSTRA<=i,]),"=",round(NROW(data_set[data_set[,TARGET]==1 & data_set$ID_AMOSTRA<=i,])/NROW(data_set[data_set$ID_AMOSTRA<=i,])*100,2),"%")
  dtini<-Sys.time()
  classifier <- randomForest(x = data_set[data_set$ID_AMOSTRA<=i,TRAIN_VECTOR],y = factor(ifelse(data_set[data_set$ID_AMOSTRA<=i,TARGET]==1, 'Y', 'N')),keep.forest=TRUE)#,importance=TRUE)
  dtfim<-Sys.time()
  y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
  resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
  intervalo<-paste0(dtini,' ~ ',dtfim)
  comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))
} 


#Criando amostra sintética de forma distribuída
data_set<-syntetic_sampling(data_set,TARGET=TARGET,n_groupby_tg = 5,groupby = c('cluster_principal'))
tipo_amostragem<-paste0("Syntetic by Cluster ",NROW(data_set[data_set[,TARGET]==1,]), "/",NROW(data_set),"=",round(NROW(data_set[data_set[,TARGET]==1,])/NROW(data_set)*100,2),"%")

dtini<-Sys.time()
classifier <- randomForest(x = data_set[,TRAIN_VECTOR],y = factor(ifelse(data_set[,TARGET]==1, 'Y', 'N')),mtry =5,ntree = 500,keep.forest=TRUE)#,importance=TRUE)
dtfim<-Sys.time()
y_pred <- predict(classifier, newdata = dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TRAIN_VECTOR],type="prob")
resumo_temp<-nfold_check(dataset_amostra[complete.cases(dataset_amostra[,TRAIN_VECTOR]),TARGET],y_pred[,"Y"],infoextra=paste0(dtini,' ~ ',dtfim), infoextra2=tipo_amostragem)
intervalo<-paste0(dtini,' ~ ',dtfim)
comparacao_dados<-rbind(comparacao_dados,cbind(resumo_temp,tipo_amostragem,intervalo))


#Verificação da importancia das variáveis do modelo
importance(classifier)
pdf('ImportanciaRF.pdf')
plot(varImpPlot(classifier))
dev.off() 

print(comparacao_dados[comparacao_dados$Acuracia>0,])
